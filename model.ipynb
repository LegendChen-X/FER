{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "316SIy2jDSrx",
        "outputId": "90b45e2e-1f37-4969-fc4f-cc8e8474ad22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ0kx4A6eIDE",
        "outputId": "32e0ca1b-a2a1-4d0a-deca-068a20725ed5"
      },
      "source": [
        "cd drive/MyDrive/csc413/"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/csc413/'\n",
            "/content/drive/MyDrive/csc413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWFNIHTweTaE",
        "outputId": "bd91e852-0021-45c1-e3bb-1d5880af2c86"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNjd7Alj-Nfv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch, os\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from util import *\n",
        "from main import *"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBv5xrjC-Ulg"
      },
      "source": [
        "classes = {\n",
        "    0: 'Surprise', 1: 'Fear', 2: 'Disgust', 3: 'Happy', 4: 'Sad', 5: 'Anger', 6: 'Neutral'\n",
        "}"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1NZAUaiefum"
      },
      "source": [
        "def conv_block(in_chnl, out_chnl, pool=False, padding=1):\n",
        "    layers = []\n",
        "    layers.append(nn.Conv2d(in_chnl, out_chnl, kernel_size=3, padding=padding))\n",
        "    layers.append(nn.BatchNorm2d(out_chnl))\n",
        "    layers.append(nn.ReLU(inplace=True))\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wS3mAIn-heD"
      },
      "source": [
        "class Base(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "    \n",
        "    def validating(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJd1HKZu-yBz"
      },
      "source": [
        "class ResNet(Base):\n",
        "    def __init__(self, in_chnls, num_cls):\n",
        "        super().__init__()\n",
        "        self.conv1 = conv_block(in_chnls, 64, pool=True)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.resnet1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.resnet2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(3), nn.Flatten(), nn.Linear(512, num_cls))\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.resnet1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.resnet2(out) + out\n",
        "        return self.classifier(out)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol0NRrAvesC6"
      },
      "source": [
        "class VGG(Base):\n",
        "    def __init__(self, in_chnls, num_cls):\n",
        "        super(VGG, self).__init__()\n",
        "        vgg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
        "        layers = []\n",
        "        in_channels = in_chnls\n",
        "        for x in vgg:\n",
        "            if x == 'M':\n",
        "                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "            else:\n",
        "                layers.append(nn.Conv2d(in_channels, x, kernel_size=3, padding=1))\n",
        "                layers.append(nn.BatchNorm2d(x))\n",
        "                layers.append(nn.ReLU(inplace=True))\n",
        "                in_channels = x\n",
        "        layers.append(nn.AvgPool2d(kernel_size=1, stride=1))\n",
        "        self.features = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(512, num_cls)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.dropout(out, p=0.5, training=self.training)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02yDWRb_E2KS"
      },
      "source": [
        "def get_default_device():\n",
        "    if torch.cuda.is_available(): return torch.device('cuda')\n",
        "    else: return torch.device('cpu')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06BVwp5Feuxj"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, images, labels, transforms):\n",
        "        self.inputs = images\n",
        "        self.labels = labels\n",
        "        self.transforms = transforms\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        data = self.inputs[i]\n",
        "        data = np.asarray(data).astype(np.uint8).reshape(48,48,1)\n",
        "        data = self.transforms(data)\n",
        "        label = self.labels[i]\n",
        "        return (data, label)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA_7lZ7l-y2R"
      },
      "source": [
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n",
        "        "
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkh1ES9o-_X_"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    #print(model)\n",
        "    outputs = [model.validating(batch) for batch in val_loader]\n",
        "    batch_losses = [x['val_loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()\n",
        "    batch_accs = [x['val_acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()\n",
        "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvALCtzW_DeT"
      },
      "source": [
        "def fit(epochs, max_lr, model, train_loader, val_loader, weight_decay, grad_clip, opt_func=torch.optim.Adam):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            if grad_clip: nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "        history.append(result)\n",
        "    return history"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dywv85MZ_Gxf"
      },
      "source": [
        "def plot_lrs(history):\n",
        "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
        "    plt.figure()\n",
        "    plt.plot(lrs)\n",
        "    plt.xlabel('Batch no.')\n",
        "    plt.ylabel('Learning rate')\n",
        "    plt.title('Learning Rate vs. Batch no.')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvwA2Yr-gGIB"
      },
      "source": [
        "def testBias(t, name):\n",
        "    data = np.load(\"../data/test.npz\")\n",
        "    model = None\n",
        "    if t==\"VGG\":\n",
        "        print(\"VGG\")\n",
        "        model = VGG(1, 7)\n",
        "    else:\n",
        "        model = ResNet(1, 7)\n",
        "    \n",
        "    model.load_state_dict(torch.load(name + \".pth\", map_location=get_default_device()))\n",
        "    model.cuda()\n",
        "    res = []\n",
        "\n",
        "    valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "    batch_num = 120\n",
        "\n",
        "    for i in range(7):\n",
        "        test_images = data[\"test_images\"+str(i)]\n",
        "        test_labels = data[\"test_labels\"+str(i)]\n",
        "        valid_data = Dataset(test_images, test_labels, valid_transform)\n",
        "        validDataLoader = DataLoader(valid_data, batch_num*2, num_workers=4, pin_memory=True)\n",
        "        device = get_default_device()\n",
        "        validDataLoader = DeviceDataLoader(validDataLoader, device)\n",
        "        result = evaluate(model, validDataLoader)\n",
        "        print(\"Emotion [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            i, result['val_loss'], result['val_acc']))\n",
        "        res.append(result['val_acc'])\n",
        "\n",
        "    return res\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esCUuNEt_Nxt"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # New dataset with VGG model\n",
        "    data = np.load(\"../data/new_data.npz\")\n",
        "    print(\"Load the data file successfully, start training:\")\n",
        "    model_type = \"VGG\"\n",
        "    train_images = data[\"train_images\"]\n",
        "    train_labels = data[\"train_labels\"]\n",
        "    test_images = data[\"test_images\"]\n",
        "    test_labels = data[\"test_labels\"]\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5), inplace=True)\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = Dataset(train_images, train_labels, train_transform)\n",
        "    valid_data = Dataset(test_images, test_labels, valid_transform)\n",
        "    print(\"Training and validation dataset initialization finished\")\n",
        "    torch.manual_seed(19)\n",
        "    batch_num = 120\n",
        "    trainDataLoader = DataLoader(train_data, batch_num, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    validDataLoader = DataLoader(valid_data, batch_num*2, num_workers=4, pin_memory=True)\n",
        "    print(\"TrainDataLoader and validDataLoader initialization finished\")\n",
        "    device = get_default_device()\n",
        "    trainDataLoader = DeviceDataLoader(trainDataLoader, device)\n",
        "    validDataLoader = DeviceDataLoader(validDataLoader, device)\n",
        "\n",
        "    if model_type==\"VGG\":\n",
        "        model = to_device(VGG(1, 7), device)\n",
        "    else:\n",
        "        model = to_device(ResNet(1, 7), device)\n",
        "    print(\"Get the model type:\" , model_type)\n",
        "    print(model)\n",
        "    print(\"Start model evaluation:\")\n",
        "    evaluate(model, validDataLoader)\n",
        "    max_lr = 0.001\n",
        "    grad_clip = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    print(\"Start model fitting:\")\n",
        "    trainLog = fit(60, max_lr, model, trainDataLoader, validDataLoader, weight_decay, grad_clip, torch.optim.Adam)\n",
        "    torch.save(model.state_dict(), \"VGG_new\"+'.pth')\n",
        "    plot_losses(trainLog)\n",
        "    plot_lrs(trainLog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCoztMHfRSZ"
      },
      "source": [
        "    # Old dataset with VGG model\n",
        "    data = np.load(\"../data/data.npz\")\n",
        "    print(\"Load the data file successfully, start training:\")\n",
        "    model_type = \"VGG\"\n",
        "    train_images = data[\"train_images\"]\n",
        "    train_labels = data[\"train_labels\"]\n",
        "    test_images = data[\"test_images\"]\n",
        "    test_labels = data[\"test_labels\"]\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5), inplace=True)\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = Dataset(train_images, train_labels, train_transform)\n",
        "    valid_data = Dataset(test_images, test_labels, valid_transform)\n",
        "    print(\"Training and validation dataset initialization finished\")\n",
        "    torch.manual_seed(19)\n",
        "    batch_num = 120\n",
        "    trainDataLoader = DataLoader(train_data, batch_num, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    validDataLoader = DataLoader(valid_data, batch_num*2, num_workers=4, pin_memory=True)\n",
        "    print(\"TrainDataLoader and validDataLoader initialization finished\")\n",
        "    device = get_default_device()\n",
        "    trainDataLoader = DeviceDataLoader(trainDataLoader, device)\n",
        "    validDataLoader = DeviceDataLoader(validDataLoader, device)\n",
        "\n",
        "    if model_type==\"VGG\":\n",
        "        model = to_device(VGG(1, 7), device)\n",
        "    else:\n",
        "        model = to_device(ResNet(1, 7), device)\n",
        "    print(\"Get the model type:\" , model_type)\n",
        "    print(model)\n",
        "    print(\"Start model evaluation:\")\n",
        "    evaluate(model, validDataLoader)\n",
        "    max_lr = 0.001\n",
        "    grad_clip = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    print(\"Start model fitting:\")\n",
        "    trainLog = fit(60, max_lr, model, trainDataLoader, validDataLoader, weight_decay, grad_clip, torch.optim.Adam)\n",
        "    torch.save(model.state_dict(), \"VGG\"+'.pth')\n",
        "    plot_losses(trainLog)\n",
        "    plot_lrs(trainLog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XI-hLmQNSuK"
      },
      "source": [
        "    # New dataset with ResNet model.\n",
        "    data = np.load(\"../data/new_data.npz\")\n",
        "    print(\"Load the data file successfully, start training:\")\n",
        "    model_type = \"ResNet\"\n",
        "    train_images = data[\"train_images\"]\n",
        "    train_labels = data[\"train_labels\"]\n",
        "    test_images = data[\"test_images\"]\n",
        "    test_labels = data[\"test_labels\"]\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5), inplace=True)\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = Dataset(train_images, train_labels, train_transform)\n",
        "    valid_data = Dataset(test_images, test_labels, valid_transform)\n",
        "    print(\"Training and validation dataset initialization finished\")\n",
        "    torch.manual_seed(19)\n",
        "    batch_num = 120\n",
        "    trainDataLoader = DataLoader(train_data, batch_num, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    validDataLoader = DataLoader(valid_data, batch_num*2, num_workers=4, pin_memory=True)\n",
        "    print(\"TrainDataLoader and validDataLoader initialization finished\")\n",
        "    device = get_default_device()\n",
        "    trainDataLoader = DeviceDataLoader(trainDataLoader, device)\n",
        "    validDataLoader = DeviceDataLoader(validDataLoader, device)\n",
        "\n",
        "    if model_type==\"VGG\":\n",
        "        model = to_device(VGG(1, 7), device)\n",
        "    else:\n",
        "        model = to_device(ResNet(1, 7), device)\n",
        "    print(\"Get the model type:\" , model_type)\n",
        "    print(model)\n",
        "    print(\"Start model evaluation:\")\n",
        "    evaluate(model, validDataLoader)\n",
        "    max_lr = 0.001\n",
        "    grad_clip = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    print(\"Start model fitting:\")\n",
        "    trainLog = fit(6, max_lr, model, trainDataLoader, validDataLoader, weight_decay, grad_clip, torch.optim.Adam)\n",
        "    torch.save(model.state_dict(), 'ResNet_new'+'.pth')\n",
        "    plot_losses(trainLog)\n",
        "    plot_lrs(trainLog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh5dPG2kj6w4"
      },
      "source": [
        "    # Old dataset with ResNet model\n",
        "    data = np.load(\"../data/data.npz\")\n",
        "    print(\"Load the data file successfully, start training:\")\n",
        "    model_type = \"ResNet\"\n",
        "    train_images = data[\"train_images\"]\n",
        "    train_labels = data[\"train_labels\"]\n",
        "    test_images = data[\"test_images\"]\n",
        "    test_labels = data[\"test_labels\"]\n",
        "\n",
        "    train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.RandomCrop(48, padding=4, padding_mode='reflect'),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5), inplace=True)\n",
        "    ])\n",
        "\n",
        "    valid_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))\n",
        "    ])\n",
        "\n",
        "    train_data = Dataset(train_images, train_labels, train_transform)\n",
        "    valid_data = Dataset(test_images, test_labels, valid_transform)\n",
        "    print(\"Training and validation dataset initialization finished\")\n",
        "    torch.manual_seed(19)\n",
        "    batch_num = 120\n",
        "    trainDataLoader = DataLoader(train_data, batch_num, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    validDataLoader = DataLoader(valid_data, batch_num*2, num_workers=4, pin_memory=True)\n",
        "    print(\"TrainDataLoader and validDataLoader initialization finished\")\n",
        "    device = get_default_device()\n",
        "    trainDataLoader = DeviceDataLoader(trainDataLoader, device)\n",
        "    validDataLoader = DeviceDataLoader(validDataLoader, device)\n",
        "\n",
        "    if model_type==\"VGG\":\n",
        "        model = to_device(VGG(1, 7), device)\n",
        "    else:\n",
        "        model = to_device(ResNet(1, 7), device)\n",
        "    print(\"Get the model type:\" , model_type)\n",
        "    print(model)\n",
        "    print(\"Start model evaluation:\")\n",
        "    evaluate(model, validDataLoader)\n",
        "    max_lr = 0.001\n",
        "    grad_clip = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    print(\"Start model fitting:\")\n",
        "    trainLog = fit(40, max_lr, model, trainDataLoader, validDataLoader, weight_decay, grad_clip, torch.optim.Adam)\n",
        "    torch.save(model.state_dict(), 'ResNet'+'.pth')\n",
        "    plot_losses(trainLog)\n",
        "    plot_lrs(trainLog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN17aTnNkK-O",
        "outputId": "a3d368d0-7890-4d94-f882-ee53c83a5a3e"
      },
      "source": [
        "    t = \"ResNet\"\n",
        "    res = testBias(t, t)\n",
        "    print(res)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Emotion [0], val_loss: 0.5258, val_acc: 0.8587\n",
            "Emotion [1], val_loss: 5.6634, val_acc: 0.3103\n",
            "Emotion [2], val_loss: 3.2309, val_acc: 0.4709\n",
            "Emotion [3], val_loss: 0.3075, val_acc: 0.9158\n",
            "Emotion [4], val_loss: 0.9783, val_acc: 0.7331\n",
            "Emotion [5], val_loss: 1.2413, val_acc: 0.7191\n",
            "Emotion [6], val_loss: 0.1838, val_acc: 0.9214\n",
            "[0.8587499856948853, 0.3103448152542114, 0.47089946269989014, 0.9158332943916321, 0.7331270575523376, 0.7191011309623718, 0.9213973879814148]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-pEg-GyfN-3",
        "outputId": "11fb5762-f342-4ee6-bf53-8ba86dc8a1bc"
      },
      "source": [
        "    t = \"ResNet\"\n",
        "    name = \"ResNet_new\"\n",
        "    res = testBias(t, name)\n",
        "    print(res)    "
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Emotion [0], val_loss: 0.5604, val_acc: 0.8846\n",
            "Emotion [1], val_loss: 5.7282, val_acc: 0.3362\n",
            "Emotion [2], val_loss: 3.9904, val_acc: 0.4709\n",
            "Emotion [3], val_loss: 1.0110, val_acc: 0.7658\n",
            "Emotion [4], val_loss: 1.1366, val_acc: 0.7438\n",
            "Emotion [5], val_loss: 1.2221, val_acc: 0.7528\n",
            "Emotion [6], val_loss: 0.2845, val_acc: 0.9214\n",
            "[0.8845833539962769, 0.3362068831920624, 0.47089946269989014, 0.76583331823349, 0.743811845779419, 0.7528089880943298, 0.9213973879814148]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSRw401r64CB",
        "outputId": "e5568457-e979-4aaa-824e-4dc4be78e2fc"
      },
      "source": [
        "t = \"VGG\"\n",
        "res = testBias(t, t)\n",
        "print(res)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Emotion [0], val_loss: 0.7832, val_acc: 0.8467\n",
            "Emotion [1], val_loss: 6.1306, val_acc: 0.3448\n",
            "Emotion [2], val_loss: 3.7729, val_acc: 0.4709\n",
            "Emotion [3], val_loss: 0.4480, val_acc: 0.9075\n",
            "Emotion [4], val_loss: 1.0825, val_acc: 0.7798\n",
            "Emotion [5], val_loss: 1.1203, val_acc: 0.7921\n",
            "Emotion [6], val_loss: 0.3617, val_acc: 0.9170\n",
            "[0.846666693687439, 0.3448275923728943, 0.47089946269989014, 0.9074999690055847, 0.7797648310661316, 0.7921348214149475, 0.9170305728912354]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5uq51zQkQAs",
        "outputId": "813125d7-f39d-4efc-e941-3055840970e2"
      },
      "source": [
        "t = \"VGG\"\n",
        "name = \"VGG_new\"\n",
        "res = testBias(t, name)\n",
        "print(res)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Emotion [0], val_loss: 0.5033, val_acc: 0.8987\n",
            "Emotion [1], val_loss: 5.6059, val_acc: 0.3448\n",
            "Emotion [2], val_loss: 3.8129, val_acc: 0.4868\n",
            "Emotion [3], val_loss: 1.6241, val_acc: 0.7683\n",
            "Emotion [4], val_loss: 1.1567, val_acc: 0.7998\n",
            "Emotion [5], val_loss: 1.2320, val_acc: 0.7921\n",
            "Emotion [6], val_loss: 0.3220, val_acc: 0.9258\n",
            "[0.8987499475479126, 0.3448275923728943, 0.48677247762680054, 0.7683333158493042, 0.7998143434524536, 0.7921348214149475, 0.9257642030715942]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}